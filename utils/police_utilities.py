#!/usr/bin/env python3
"""
üèõÔ∏è POLICE CYBER MONITORING SYSTEM - CORE UTILITIES
Professional utility functions for law enforcement cyber intelligence operations
Optimized for large-scale social media monitoring and threat analysis
"""

import re
import json
import csv
import logging
import hashlib
import time
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any, Union
from collections import defaultdict, Counter
import xml.etree.ElementTree as ET

# Try to import optional dependencies with fallbacks
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

try:
    from langdetect import detect, LangDetectError
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False

class PoliceUtilities:
    """
    üö® Professional utility class for police cyber monitoring operations
    Provides essential functions for text processing, analysis, and evidence management
    """
    
    def __init__(self, log_level: str = "INFO"):
        """Initialize police utilities with logging configuration"""
        
        # Setup logging system
        self.logger = self._setup_logging(log_level)
        
        # Indian city and state patterns for geographic extraction
        self.indian_locations = {
            'cities': [
                'mumbai', 'delhi', 'bangalore', 'hyderabad', 'ahmedabad', 'chennai',
                'kolkata', 'surat', 'pune', 'jaipur', 'lucknow', 'kanpur', 'nagpur',
                'indore', 'thane', 'bhopal', 'visakhapatnam', 'pimpri', 'patna',
                'vadodara', 'ghaziabad', 'ludhiana', 'agra', 'nashik', 'faridabad',
                'meerut', 'rajkot', 'kalyan', 'vasai', 'varanasi', 'srinagar',
                'aurangabad', 'dhanbad', 'amritsar', 'navi mumbai', 'allahabad',
                'howrah', 'ranchi', 'gwalior', 'jabalpur', 'coimbatore'
            ],
            'states': [
                'maharashtra', 'uttar pradesh', 'bihar', 'west bengal', 'madhya pradesh',
                'tamil nadu', 'rajasthan', 'karnataka', 'gujarat', 'andhra pradesh',
                'odisha', 'telangana', 'kerala', 'jharkhand', 'assam', 'punjab',
                'chhattisgarh', 'haryana', 'jammu and kashmir', 'ladakh', 'uttarakhand',
                'himachal pradesh', 'tripura', 'meghalaya', 'manipur', 'nagaland',
                'goa', 'arunachal pradesh', 'mizoram', 'sikkim', 'delhi'
            ]
        }
        
        # Threat keywords for police operations
        self.threat_keywords = {
            'terrorism': [
                'terrorism', 'terrorist', 'bomb', 'blast', 'attack', 'jihad',
                'isis', 'al qaeda', 'taliban', 'explosive', 'suicide bomber'
            ],
            'cyber_crime': [
                'hacking', 'phishing', 'fraud', 'scam', 'malware', 'ransomware',
                'cyber attack', 'data breach', 'identity theft', 'online fraud'
            ],
            'anti_national': [
                'anti national', 'separatist', 'sedition', 'treason', 'conspiracy',
                'overthrow government', 'anti india', 'break india'
            ],
            'drugs': [
                'drug trafficking', 'narcotics', 'cocaine', 'heroin', 'opium',
                'smuggling', 'drug dealer', 'contraband'
            ]
        }
        
        # Performance monitoring data
        self.performance_stats = {
            'function_calls': defaultdict(int),
            'processing_times': defaultdict(list),
            'error_counts': defaultdict(int),
            'data_volumes': defaultdict(list)
        }
        
        self.logger.info("üèõÔ∏è Police Utilities initialized for cyber monitoring operations")
    
    def text_preprocessing(self, text: str, remove_urls: bool = True, 
                          remove_mentions: bool = False, normalize_case: bool = True) -> str:
        """
        üîß Clean and normalize text for police analysis
        
        Args:
            text: Raw text to process
            remove_urls: Remove URL links
            remove_mentions: Remove @mentions
            normalize_case: Convert to lowercase
            
        Returns:
            Cleaned and normalized text
        """
        
        start_time = time.time()
        self.performance_stats['function_calls']['text_preprocessing'] += 1
        
        try:
            if not text or not isinstance(text, str):
                self.logger.warning("Invalid text input for preprocessing")
                return ""
            
            # Store original length for performance tracking
            original_length = len(text)
            
            # Remove extra whitespace
            text = re.sub(r'\s+', ' ', text.strip())
            
            # Remove URLs if requested
            if remove_urls:
                text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
                text = re.sub(r'www\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
            
            # Remove @mentions if requested
            if remove_mentions:
                text = re.sub(r'@\w+', '', text)
            
            # Remove special characters but keep basic punctuation
            text = re.sub(r'[^\w\s\.\!\?\,\;\:\-\(\)]', ' ', text)
            
            # Normalize case if requested
            if normalize_case:
                text = text.lower()
            
            # Remove extra spaces again
            text = re.sub(r'\s+', ' ', text.strip())
            
            # Log processing statistics
            processing_time = time.time() - start_time
            self.performance_stats['processing_times']['text_preprocessing'].append(processing_time)
            self.performance_stats['data_volumes']['text_preprocessing'].append(original_length)
            
            self.logger.debug(f"Text preprocessing completed: {original_length} -> {len(text)} chars in {processing_time:.3f}s")
            
            return text
            
        except Exception as e:
            self.performance_stats['error_counts']['text_preprocessing'] += 1
            self.logger.error(f"Error in text preprocessing: {e}")
            return text if isinstance(text, str) else ""
    
    def language_detection(self, text: str) -> Dict[str, Any]:
        """
        üîç Identify language of text (Hindi/English/other)
        
        Args:
            text: Text to analyze
            
        Returns:
            Dictionary with language, confidence, and details
        """
        
        start_time = time.time()
        self.performance_stats['function_calls']['language_detection'] += 1
        
        try:
            if not text or len(text.strip()) < 3:
                return {
                    'language': 'unknown',
                    'confidence': 0.0,
                    'method': 'insufficient_text',
                    'details': 'Text too short for detection'
                }
            
            result = {
                'language': 'unknown',
                'confidence': 0.0,
                'method': 'pattern_based',
                'details': {}
            }
            
            # Clean text for analysis
            clean_text = self.text_preprocessing(text, remove_urls=True, normalize_case=False)
            
            # Pattern-based detection for Hindi (Devanagari script)
            hindi_chars = len(re.findall(r'[\u0900-\u097F]', clean_text))
            english_chars = len(re.findall(r'[a-zA-Z]', clean_text))
            total_chars = len(re.sub(r'\s', '', clean_text))
            
            if total_chars == 0:
                return result
            
            hindi_ratio = hindi_chars / total_chars
            english_ratio = english_chars / total_chars
            
            # Determine language based on character ratios
            if hindi_ratio > 0.3:
                result['language'] = 'hindi'
                result['confidence'] = min(hindi_ratio * 2, 1.0)
                result['details'] = {
                    'hindi_chars': hindi_chars,
                    'english_chars': english_chars,
                    'hindi_ratio': hindi_ratio
                }
            elif english_ratio > 0.7:
                result['language'] = 'english'
                result['confidence'] = min(english_ratio, 1.0)
                result['details'] = {
                    'english_chars': english_chars,
                    'hindi_chars': hindi_chars,
                    'english_ratio': english_ratio
                }
            else:
                result['language'] = 'mixed'
                result['confidence'] = 0.5
                result['details'] = {
                    'hindi_chars': hindi_chars,
                    'english_chars': english_chars,
                    'mixed_content': True
                }
            
            # Use langdetect library if available for enhanced detection
            if LANGDETECT_AVAILABLE and len(clean_text) > 20:
                try:
                    detected_lang = detect(clean_text)
                    if detected_lang in ['hi', 'en']:
                        result['method'] = 'langdetect_enhanced'
                        if detected_lang == 'hi':
                            result['language'] = 'hindi'
                        elif detected_lang == 'en':
                            result['language'] = 'english'
                        result['confidence'] = min(result['confidence'] + 0.2, 1.0)
                        result['details']['langdetect_result'] = detected_lang
                except LangDetectError:
                    pass
            
            # Log processing statistics
            processing_time = time.time() - start_time
            self.performance_stats['processing_times']['language_detection'].append(processing_time)
            
            self.logger.debug(f"Language detection: {result['language']} (confidence: {result['confidence']:.2f})")
            
            return result
            
        except Exception as e:
            self.performance_stats['error_counts']['language_detection'] += 1
            self.logger.error(f"Error in language detection: {e}")
            return {
                'language': 'error',
                'confidence': 0.0,
                'method': 'error',
                'details': {'error': str(e)}
            }
    
    def geographic_extraction(self, text: str) -> Dict[str, Any]:
        """
        üó∫Ô∏è Extract location mentions from text (Indian focus)
        
        Args:
            text: Text to analyze for locations
            
        Returns:
            Dictionary with found locations and geographic data
        """
        
        start_time = time.time()
        self.performance_stats['function_calls']['geographic_extraction'] += 1
        
        try:
            if not text:
                return {'cities': [], 'states': [], 'total_locations': 0, 'confidence': 0.0}
            
            # Clean and normalize text
            clean_text = self.text_preprocessing(text, remove_urls=True, normalize_case=True)
            
            found_locations = {
                'cities': [],
                'states': [],
                'coordinates': [],
                'other_locations': []
            }
            
            # Search for Indian cities
            for city in self.indian_locations['cities']:
                pattern = r'\b' + re.escape(city.lower()) + r'\b'
                if re.search(pattern, clean_text):
                    found_locations['cities'].append({
                        'name': city.title(),
                        'confidence': 0.9,
                        'context': self._extract_context(clean_text, city.lower())
                    })
            
            # Search for Indian states
            for state in self.indian_locations['states']:
                pattern = r'\b' + re.escape(state.lower()) + r'\b'
                if re.search(pattern, clean_text):
                    found_locations['states'].append({
                        'name': state.title(),
                        'confidence': 0.95,
                        'context': self._extract_context(clean_text, state.lower())
                    })
            
            # Search for coordinate patterns
            coord_pattern = r'(\d{1,2}\.\d+)[¬∞\s]*[NS][\s,]*(\d{1,3}\.\d+)[¬∞\s]*[EW]'
            coordinates = re.findall(coord_pattern, clean_text)
            for lat, lon in coordinates:
                found_locations['coordinates'].append({
                    'latitude': float(lat),
                    'longitude': float(lon),
                    'confidence': 0.8
                })
            
            # Search for other location indicators
            location_keywords = ['near', 'at', 'in', 'from', 'location', 'place', 'area']
            for keyword in location_keywords:
                pattern = r'\b' + keyword + r'\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)'
                matches = re.findall(pattern, text)
                for match in matches:
                    if match.lower() not in [city.lower() for city in self.indian_locations['cities']]:
                        found_locations['other_locations'].append({
                            'name': match,
                            'confidence': 0.6,
                            'keyword': keyword
                        })
            
            # Calculate overall confidence and metrics
            total_locations = (len(found_locations['cities']) + 
                             len(found_locations['states']) + 
                             len(found_locations['coordinates']))
            
            confidence = 0.0
            if total_locations > 0:
                confidence = min(total_locations * 0.3, 1.0)
            
            result = {
                'cities': found_locations['cities'],
                'states': found_locations['states'],
                'coordinates': found_locations['coordinates'],
                'other_locations': found_locations['other_locations'],
                'total_locations': total_locations,
                'confidence': confidence,
                'geographic_focus': 'india'
            }
            
            # Log processing statistics
            processing_time = time.time() - start_time
            self.performance_stats['processing_times']['geographic_extraction'].append(processing_time)
            
            self.logger.debug(f"Geographic extraction found {total_locations} locations")
            
            return result
            
        except Exception as e:
            self.performance_stats['error_counts']['geographic_extraction'] += 1
            self.logger.error(f"Error in geographic extraction: {e}")
            return {'cities': [], 'states': [], 'total_locations': 0, 'confidence': 0.0, 'error': str(e)}
    
    def time_analysis(self, timestamps: List[Union[str, datetime]], 
                     timezone: str = 'Asia/Kolkata') -> Dict[str, Any]:
        """
        ‚è∞ Analyze posting patterns and temporal behavior
        
        Args:
            timestamps: List of timestamp strings or datetime objects
            timezone: Timezone for analysis (default: Indian Standard Time)
            
        Returns:
            Dictionary with temporal analysis results
        """
        
        start_time = time.time()
        self.performance_stats['function_calls']['time_analysis'] += 1
        
        try:
            if not timestamps:
                return {'error': 'No timestamps provided'}
            
            # Convert timestamps to datetime objects
            dt_list = []
            for ts in timestamps:
                try:
                    if isinstance(ts, str):
                        # Try multiple datetime formats
                        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%d']:
                            try:
                                dt_list.append(datetime.strptime(ts, fmt))
                                break
                            except ValueError:
                                continue
                    elif isinstance(ts, datetime):
                        dt_list.append(ts)
                except:
                    continue
            
            if not dt_list:
                return {'error': 'No valid timestamps found'}
            
            # Sort timestamps
            dt_list.sort()
            
            # Analyze patterns
            analysis = {
                'total_posts': len(dt_list),
                'time_span': {
                    'start': dt_list[0].isoformat(),
                    'end': dt_list[-1].isoformat(),
                    'duration_hours': (dt_list[-1] - dt_list[0]).total_seconds() / 3600
                },
                'hourly_distribution': defaultdict(int),
                'daily_distribution': defaultdict(int),
                'posting_velocity': {},
                'suspicious_patterns': []
            }
            
            # Analyze hourly patterns
            for dt in dt_list:
                analysis['hourly_distribution'][dt.hour] += 1
                analysis['daily_distribution'][dt.strftime('%Y-%m-%d')] += 1
            
            # Calculate posting velocity (posts per hour)
            if len(dt_list) > 1:
                total_hours = analysis['time_span']['duration_hours']
                analysis['posting_velocity'] = {
                    'posts_per_hour': len(dt_list) / max(total_hours, 1),
                    'posts_per_day': len(dt_list) / max(total_hours / 24, 1),
                    'peak_hour': max(analysis['hourly_distribution'], 
                                   key=analysis['hourly_distribution'].get),
                    'peak_day': max(analysis['daily_distribution'], 
                                  key=analysis['daily_distribution'].get)
                }
            
            # Detect suspicious patterns
            hourly_counts = list(analysis['hourly_distribution'].values())
            if hourly_counts:
                avg_hourly = sum(hourly_counts) / len(hourly_counts)
                max_hourly = max(hourly_counts)
                
                # Check for bot-like regular posting
                if len(set(hourly_counts)) < 3 and len(hourly_counts) > 5:
                    analysis['suspicious_patterns'].append({
                        'type': 'regular_posting',
                        'description': 'Unusually regular posting pattern',
                        'confidence': 0.7
                    })
                
                # Check for burst activity
                if max_hourly > avg_hourly * 5:
                    analysis['suspicious_patterns'].append({
                        'type': 'burst_activity',
                        'description': 'High burst activity detected',
                        'confidence': 0.8
                    })
                
                # Check for night-time activity (Indian time)
                night_hours = list(range(23, 24)) + list(range(0, 6))
                night_posts = sum(analysis['hourly_distribution'][h] for h in night_hours)
                if night_posts > len(dt_list) * 0.3:
                    analysis['suspicious_patterns'].append({
                        'type': 'night_activity',
                        'description': 'High night-time activity',
                        'confidence': 0.6
                    })
            
            # Log processing statistics
            processing_time = time.time() - start_time
            self.performance_stats['processing_times']['time_analysis'].append(processing_time)
            
            self.logger.debug(f"Time analysis completed for {len(dt_list)} timestamps")
            
            return analysis
            
        except Exception as e:
            self.performance_stats['error_counts']['time_analysis'] += 1
            self.logger.error(f"Error in time analysis: {e}")
            return {'error': str(e)}
    
    def network_analysis(self, connections: List[Tuple[str, str]], 
                        node_attributes: Optional[Dict[str, Dict]] = None) -> Dict[str, Any]:
        """
        üï∏Ô∏è Analyze account relationship networks
        
        Args:
            connections: List of (source, target) connection tuples
            node_attributes: Optional attributes for each node
            
        Returns:
            Dictionary with network analysis results
        """
        
        start_time = time.time()
        self.performance_stats['function_calls']['network_analysis'] += 1
        
        try:
            if not connections:
                return {'error': 'No connections provided'}
            
            # Basic network analysis without NetworkX
            nodes = set()
            edges = connections
            
            for source, target in connections:
                nodes.add(source)
                nodes.add(target)
            
            # Calculate basic network metrics
            node_degrees = defaultdict(int)
            for source, target in connections:
                node_degrees[source] += 1
                node_degrees[target] += 1
            
            analysis = {
                'network_size': {
                    'total_nodes': len(nodes),
                    'total_edges': len(edges),
                    'density': len(edges) / (len(nodes) * (len(nodes) - 1)) if len(nodes) > 1 else 0
                },
                'node_metrics': {},
                'central_nodes': [],
                'suspicious_patterns': [],
                'communities': []
            }
            
            # Analyze individual nodes
            for node in nodes:
                degree = node_degrees[node]
                analysis['node_metrics'][node] = {
                    'degree': degree,
                    'degree_centrality': degree / (len(nodes) - 1) if len(nodes) > 1 else 0
                }
                
                # Add node attributes if provided
                if node_attributes and node in node_attributes:
                    analysis['node_metrics'][node].update(node_attributes[node])
            
            # Identify central nodes (high degree)
            sorted_nodes = sorted(nodes, key=lambda x: node_degrees[x], reverse=True)
            top_nodes = sorted_nodes[:min(5, len(sorted_nodes))]
            
            for node in top_nodes:
                analysis['central_nodes'].append({
                    'node': node,
                    'degree': node_degrees[node],
                    'importance': node_degrees[node] / max(node_degrees.values())
                })
            
            # Detect suspicious patterns
            avg_degree = sum(node_degrees.values()) / len(nodes) if nodes else 0
            max_degree = max(node_degrees.values()) if node_degrees else 0
            
            # Check for hub nodes (potential bot controllers)
            if max_degree > avg_degree * 5:
                hub_nodes = [node for node, degree in node_degrees.items() if degree > avg_degree * 3]
                analysis['suspicious_patterns'].append({
                    'type': 'hub_nodes',
                    'description': f'Potential bot controller nodes detected: {hub_nodes}',
                    'nodes': hub_nodes,
                    'confidence': 0.8
                })
            
            # Check for isolated clusters
            if len(edges) < len(nodes) * 0.5:
                analysis['suspicious_patterns'].append({
                    'type': 'sparse_network',
                    'description': 'Network appears fragmented with isolated groups',
                    'confidence': 0.6
                })
            
            # Enhanced analysis with NetworkX if available
            if NETWORKX_AVAILABLE:
                try:
                    G = nx.Graph()
                    G.add_edges_from(connections)
                    
                    # Calculate advanced metrics
                    analysis['advanced_metrics'] = {
                        'clustering_coefficient': nx.average_clustering(G),
                        'connected_components': nx.number_connected_components(G),
                        'diameter': nx.diameter(G) if nx.is_connected(G) else 'Not connected'
                    }
                    
                    # Detect communities
                    if len(G.nodes()) > 3:
                        try:
                            communities = list(nx.community.greedy_modularity_communities(G))
                            analysis['communities'] = [list(community) for community in communities]
                        except:
                            pass
                            
                except Exception as e:
                    self.logger.warning(f"NetworkX analysis failed: {e}")
            
            # Log processing statistics
            processing_time = time.time() - start_time
            self.performance_stats['processing_times']['network_analysis'].append(processing_time)
            
            self.logger.debug(f"Network analysis completed: {len(nodes)} nodes, {len(edges)} edges")
            
            return analysis
            
        except Exception as e:
            self.performance_stats['error_counts']['network_analysis'] += 1
            self.logger.error(f"Error in network analysis: {e}")
            return {'error': str(e)}
    
    def evidence_formatter(self, data: Dict[str, Any], case_id: str, 
                          officer_id: str, format_type: str = 'comprehensive') -> Dict[str, Any]:
        """
        ‚öñÔ∏è Format data for court-admissible evidence documentation
        
        Args:
            data: Raw analysis data to format
            case_id: Police case identifier
            officer_id: Investigating officer ID
            format_type: Type of evidence format (comprehensive, summary, technical)
            
        Returns:
            Court-ready evidence package
        """
        
        start_time = time.time()
        self.performance_stats['function_calls']['evidence_formatter'] += 1
        
        try:
            timestamp = datetime.now()
            
            evidence_package = {
                'case_information': {
                    'case_id': case_id,
                    'investigating_officer': officer_id,
                    'evidence_creation_time': timestamp.isoformat(),
                    'evidence_id': f"EVIDENCE_{case_id}_{int(timestamp.timestamp())}",
                    'jurisdiction': 'Indian Cyber Crime Investigation',
                    'legal_framework': ['IT Act 2000', 'Indian Evidence Act 1872']
                },
                'digital_evidence': {
                    'source_data': data,
                    'data_integrity': {
                        'hash_algorithm': 'SHA-256',
                        'data_hash': hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest(),
                        'verification_timestamp': timestamp.isoformat()
                    },
                    'chain_of_custody': []
                },
                'analysis_summary': {},
                'technical_details': {},
                'legal_compliance': {
                    'evidence_standards': 'Section 65B IT Act 2000',
                    'digital_signature': 'Required for court submission',
                    'authenticity_verified': True,
                    'tampering_protection': 'SHA-256 hash verification'
                }
            }
            
            # Add chain of custody entry
            evidence_package['digital_evidence']['chain_of_custody'].append({
                'action': 'Evidence Creation',
                'officer': officer_id,
                'timestamp': timestamp.isoformat(),
                'description': f'Digital evidence formatted for case {case_id}',
                'hash_verification': evidence_package['digital_evidence']['data_integrity']['data_hash']
            })
            
            # Format analysis summary based on data type
            if 'threat_score' in data:
                evidence_package['analysis_summary']['threat_assessment'] = {
                    'threat_level': data.get('threat_score', 0),
                    'risk_category': self._categorize_threat_level(data.get('threat_score', 0)),
                    'confidence': data.get('confidence', 0.0)
                }
            
            if 'language' in data:
                evidence_package['analysis_summary']['linguistic_analysis'] = {
                    'primary_language': data.get('language', 'unknown'),
                    'detection_confidence': data.get('confidence', 0.0)
                }
            
            if 'cities' in data or 'states' in data:
                evidence_package['analysis_summary']['geographic_intelligence'] = {
                    'locations_identified': data.get('total_locations', 0),
                    'cities': data.get('cities', []),
                    'states': data.get('states', [])
                }
            
            if 'network_size' in data:
                evidence_package['analysis_summary']['network_analysis'] = {
                    'network_nodes': data.get('network_size', {}).get('total_nodes', 0),
                    'network_connections': data.get('network_size', {}).get('total_edges', 0),
                    'central_nodes': data.get('central_nodes', [])
                }
            
            # Add technical details for different format types
            if format_type == 'comprehensive':
                evidence_package['technical_details'] = {
                    'processing_methodology': 'Automated police cyber analysis system',
                    'algorithms_used': ['Text preprocessing', 'Pattern recognition', 'Statistical analysis'],
                    'data_sources': 'Social media monitoring systems',
                    'analysis_timestamp': timestamp.isoformat(),
                    'system_version': '1.0.0',
                    'confidence_metrics': self._extract_confidence_metrics(data)
                }
            elif format_type == 'summary':
                evidence_package['technical_details'] = {
                    'analysis_type': 'Automated cyber intelligence analysis',
                    'timestamp': timestamp.isoformat(),
                    'confidence_level': self._calculate_overall_confidence(data)
                }
            
            # Add legal compliance notes
            evidence_package['legal_compliance']['court_submission_notes'] = [
                'Evidence created using certified police cyber monitoring system',
                'Data integrity verified through cryptographic hashing',
                'Chain of custody maintained throughout analysis process',
                'Compliant with Section 65B of IT Act 2000 for digital evidence'
            ]
            
            # Log processing statistics
            processing_time = time.time() - start_time
            self.performance_stats['processing_times']['evidence_formatter'].append(processing_time)
            
            self.logger.info(f"Evidence package created for case {case_id} by officer {officer_id}")
            
            return evidence_package
            
        except Exception as e:
            self.performance_stats['error_counts']['evidence_formatter'] += 1
            self.logger.error(f"Error in evidence formatting: {e}")
            return {'error': str(e), 'case_id': case_id}
    
    def alert_prioritization(self, alerts: List[Dict[str, Any]], 
                           max_priority_alerts: int = 10) -> List[Dict[str, Any]]:
        """
        üö® Sort and prioritize alerts based on risk assessment
        
        Args:
            alerts: List of alert dictionaries with threat data
            max_priority_alerts: Maximum number of high-priority alerts to return
            
        Returns:
            Sorted list of prioritized alerts
        """
        
        start_time = time.time()
        self.performance_stats['function_calls']['alert_prioritization'] += 1
        
        try:
            if not alerts:
                return []
            
            # Calculate priority scores for each alert
            prioritized_alerts = []
            
            for alert in alerts:
                priority_score = self._calculate_alert_priority(alert)
                
                alert_copy = alert.copy()
                alert_copy['priority_score'] = priority_score
                alert_copy['priority_level'] = self._get_priority_level(priority_score)
                alert_copy['escalation_required'] = priority_score >= 80
                alert_copy['processing_timestamp'] = datetime.now().isoformat()
                
                prioritized_alerts.append(alert_copy)
            
            # Sort by priority score (highest first)
            prioritized_alerts.sort(key=lambda x: x['priority_score'], reverse=True)
            
            # Add ranking information
            for i, alert in enumerate(prioritized_alerts):
                alert['priority_rank'] = i + 1
                alert['top_priority'] = i < max_priority_alerts
            
            # Log prioritization results
            high_priority_count = sum(1 for alert in prioritized_alerts if alert['priority_score'] >= 70)
            
            self.logger.info(f"Alert prioritization: {len(alerts)} alerts processed, "
                           f"{high_priority_count} high-priority alerts identified")
            
            # Log processing statistics
            processing_time = time.time() - start_time
            self.performance_stats['processing_times']['alert_prioritization'].append(processing_time)
            
            return prioritized_alerts
            
        except Exception as e:
            self.performance_stats['error_counts']['alert_prioritization'] += 1
            self.logger.error(f"Error in alert prioritization: {e}")
            return alerts  # Return original alerts on error
    
    def data_export(self, data: Any, filename: str, format_type: str = 'json',
                   include_metadata: bool = True) -> Dict[str, Any]:
        """
        üìÅ Export data in multiple formats for police operations
        
        Args:
            data: Data to export
            filename: Output filename (without extension)
            format_type: Export format (json, csv, xml, txt)
            include_metadata: Include export metadata
            
        Returns:
            Export result with file information
        """
        
        start_time = time.time()
        self.performance_stats['function_calls']['data_export'] += 1
        
        try:
            timestamp = datetime.now()
            export_metadata = {
                'export_timestamp': timestamp.isoformat(),
                'export_format': format_type,
                'data_size': len(str(data)),
                'police_system': 'Cyber Intelligence Monitoring'
            }
            
            # Determine file extension and path
            extensions = {
                'json': '.json',
                'csv': '.csv', 
                'xml': '.xml',
                'txt': '.txt'
            }
            
            if format_type not in extensions:
                raise ValueError(f"Unsupported format: {format_type}")
            
            file_path = f"{filename}{extensions[format_type]}"
            
            # Export data based on format
            if format_type == 'json':
                export_data = {
                    'data': data,
                    'metadata': export_metadata if include_metadata else None
                }
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, indent=2, ensure_ascii=False)
            
            elif format_type == 'csv':
                if PANDAS_AVAILABLE and isinstance(data, (list, dict)):
                    # Convert to DataFrame if possible
                    if isinstance(data, list) and data and isinstance(data[0], dict):
                        df = pd.DataFrame(data)
                        df.to_csv(file_path, index=False, encoding='utf-8')
                    else:
                        # Fallback to manual CSV writing
                        self._write_csv_manually(data, file_path, include_metadata, export_metadata)
                else:
                    self._write_csv_manually(data, file_path, include_metadata, export_metadata)
            
            elif format_type == 'xml':
                self._write_xml(data, file_path, include_metadata, export_metadata)
            
            elif format_type == 'txt':
                with open(file_path, 'w', encoding='utf-8') as f:
                    if include_metadata:
                        f.write("POLICE CYBER INTELLIGENCE EXPORT\n")
                        f.write("=" * 50 + "\n")
                        f.write(f"Export Time: {export_metadata['export_timestamp']}\n")
                        f.write(f"Data Size: {export_metadata['data_size']} characters\n")
                        f.write("=" * 50 + "\n\n")
                    
                    if isinstance(data, dict):
                        f.write(json.dumps(data, indent=2, ensure_ascii=False))
                    else:
                        f.write(str(data))
            
            # Calculate file size
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            
            result = {
                'success': True,
                'file_path': file_path,
                'file_size_bytes': file_size,
                'format': format_type,
                'export_metadata': export_metadata,
                'data_integrity_hash': hashlib.sha256(str(data).encode()).hexdigest()
            }
            
            # Log processing statistics
            processing_time = time.time() - start_time
            self.performance_stats['processing_times']['data_export'].append(processing_time)
            
            self.logger.info(f"Data exported successfully: {file_path} ({file_size} bytes)")
            
            return result
            
        except Exception as e:
            self.performance_stats['error_counts']['data_export'] += 1
            self.logger.error(f"Error in data export: {e}")
            return {
                'success': False,
                'error': str(e),
                'file_path': filename,
                'format': format_type
            }
    
    def security_logger(self, event_type: str, details: Dict[str, Any],
                       user_id: Optional[str] = None, severity: str = 'INFO') -> bool:
        """
        üîí Maintain security audit trail for police operations
        
        Args:
            event_type: Type of security event
            details: Event details dictionary
            user_id: User/officer ID if applicable
            severity: Log severity level
            
        Returns:
            True if logged successfully
        """
        
        try:
            timestamp = datetime.now()
            
            security_event = {
                'timestamp': timestamp.isoformat(),
                'event_type': event_type,
                'severity': severity,
                'user_id': user_id,
                'details': details,
                'system': 'Police Cyber Monitoring',
                'session_id': self._generate_session_id(),
                'integrity_hash': hashlib.sha256(
                    f"{timestamp.isoformat()}{event_type}{str(details)}".encode()
                ).hexdigest()
            }
            
            # Log to security log file
            security_log_file = 'police_security_audit.log'
            
            with open(security_log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(security_event) + '\n')
            
            # Also log through standard logger based on severity
            if severity == 'CRITICAL':
                self.logger.critical(f"Security Event: {event_type} - {details}")
            elif severity == 'ERROR':
                self.logger.error(f"Security Event: {event_type} - {details}")
            elif severity == 'WARNING':
                self.logger.warning(f"Security Event: {event_type} - {details}")
            else:
                self.logger.info(f"Security Event: {event_type} - {details}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to log security event: {e}")
            return False
    
    def performance_monitor(self) -> Dict[str, Any]:
        """
        üìä Monitor system performance and health
        
        Returns:
            System performance metrics and health status
        """
        
        try:
            current_time = datetime.now()
            
            # Calculate performance statistics
            performance_report = {
                'timestamp': current_time.isoformat(),
                'system_health': 'healthy',
                'function_statistics': {},
                'error_rates': {},
                'processing_efficiency': {},
                'resource_usage': {},
                'recommendations': []
            }
            
            # Analyze function call statistics
            for func_name, call_count in self.performance_stats['function_calls'].items():
                if call_count > 0:
                    times = self.performance_stats['processing_times'][func_name]
                    errors = self.performance_stats['error_counts'][func_name]
                    
                    avg_time = sum(times) / len(times) if times else 0
                    error_rate = errors / call_count if call_count > 0 else 0
                    
                    performance_report['function_statistics'][func_name] = {
                        'total_calls': call_count,
                        'total_errors': errors,
                        'average_processing_time': avg_time,
                        'error_rate_percentage': error_rate * 100
                    }
                    
                    performance_report['error_rates'][func_name] = error_rate
                    
                    # Efficiency analysis
                    data_volumes = self.performance_stats['data_volumes'][func_name]
                    if data_volumes and times:
                        avg_data_size = sum(data_volumes) / len(data_volumes)
                        throughput = avg_data_size / avg_time if avg_time > 0 else 0
                        
                        performance_report['processing_efficiency'][func_name] = {
                            'average_data_size': avg_data_size,
                            'throughput_per_second': throughput
                        }
            
            # System health assessment
            overall_error_rate = sum(self.performance_stats['error_counts'].values()) / max(
                sum(self.performance_stats['function_calls'].values()), 1
            )
            
            if overall_error_rate > 0.1:
                performance_report['system_health'] = 'warning'
                performance_report['recommendations'].append(
                    'High error rate detected - investigate system issues'
                )
            elif overall_error_rate > 0.05:
                performance_report['system_health'] = 'caution'
                performance_report['recommendations'].append(
                    'Moderate error rate - monitor system performance'
                )
            
            # Performance recommendations
            for func_name, stats in performance_report['function_statistics'].items():
                if stats['average_processing_time'] > 5.0:
                    performance_report['recommendations'].append(
                        f'Function {func_name} has high processing time - consider optimization'
                    )
                
                if stats['error_rate_percentage'] > 10:
                    performance_report['recommendations'].append(
                        f'Function {func_name} has high error rate - investigate issues'
                    )
            
            # Resource usage (basic metrics)
            performance_report['resource_usage'] = {
                'total_function_calls': sum(self.performance_stats['function_calls'].values()),
                'total_processing_time': sum([
                    sum(times) for times in self.performance_stats['processing_times'].values()
                ]),
                'memory_usage': 'Not available (requires psutil)',
                'cpu_usage': 'Not available (requires psutil)'
            }
            
            self.logger.info(f"Performance monitoring completed - Health: {performance_report['system_health']}")
            
            return performance_report
            
        except Exception as e:
            self.logger.error(f"Error in performance monitoring: {e}")
            return {
                'timestamp': datetime.now().isoformat(),
                'system_health': 'error',
                'error': str(e)
            }
    
    # Helper methods
    def _setup_logging(self, log_level: str) -> logging.Logger:
        """Setup logging configuration for police utilities"""
        
        logger = logging.getLogger('PoliceUtilities')
        logger.setLevel(getattr(logging, log_level.upper()))
        
        # Create file handler for police logs
        log_filename = f"police_utilities_{datetime.now().strftime('%Y%m%d')}.log"
        file_handler = logging.FileHandler(log_filename)
        file_handler.setLevel(logging.INFO)
        
        # Create console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.WARNING)
        
        # Create formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # Add handlers to logger
        if not logger.handlers:
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)
        
        return logger
    
    def _extract_context(self, text: str, term: str, context_length: int = 50) -> str:
        """Extract context around a found term"""
        
        term_index = text.lower().find(term.lower())
        if term_index == -1:
            return ""
        
        start = max(0, term_index - context_length)
        end = min(len(text), term_index + len(term) + context_length)
        
        return text[start:end].strip()
    
    def _categorize_threat_level(self, threat_score: float) -> str:
        """Categorize threat level based on score"""
        
        if threat_score >= 80:
            return "CRITICAL"
        elif threat_score >= 60:
            return "HIGH"
        elif threat_score >= 40:
            return "MEDIUM"
        elif threat_score >= 20:
            return "LOW"
        else:
            return "MINIMAL"
    
    def _calculate_alert_priority(self, alert: Dict[str, Any]) -> float:
        """Calculate priority score for an alert"""
        
        priority_score = 0.0
        
        # Base threat score
        threat_score = alert.get('threat_score', 0)
        priority_score += threat_score * 0.4
        
        # Geographic factor (higher priority for sensitive locations)
        if 'location' in alert:
            location = alert['location'].lower()
            sensitive_locations = ['delhi', 'mumbai', 'parliament', 'airport', 'military']
            if any(loc in location for loc in sensitive_locations):
                priority_score += 15
        
        # Keyword severity
        if 'keywords' in alert:
            keywords = [kw.lower() for kw in alert['keywords']]
            high_risk_keywords = ['bomb', 'attack', 'terrorism', 'explosive']
            critical_matches = sum(1 for kw in keywords if kw in high_risk_keywords)
            priority_score += critical_matches * 10
        
        # Network analysis factor
        if 'network_size' in alert:
            network_size = alert['network_size']
            if network_size > 50:
                priority_score += 10
            elif network_size > 20:
                priority_score += 5
        
        # Time sensitivity
        if 'timestamp' in alert:
            try:
                alert_time = datetime.fromisoformat(alert['timestamp'])
                age_hours = (datetime.now() - alert_time).total_seconds() / 3600
                
                # Recent alerts get higher priority
                if age_hours < 1:
                    priority_score += 10
                elif age_hours < 6:
                    priority_score += 5
            except:
                pass
        
        return min(priority_score, 100.0)
    
    def _get_priority_level(self, priority_score: float) -> str:
        """Get priority level label based on score"""
        
        if priority_score >= 80:
            return "CRITICAL"
        elif priority_score >= 60:
            return "HIGH"
        elif priority_score >= 40:
            return "MEDIUM"
        else:
            return "LOW"
    
    def _extract_confidence_metrics(self, data: Dict[str, Any]) -> Dict[str, float]:
        """Extract confidence metrics from analysis data"""
        
        confidence_metrics = {}
        
        if 'confidence' in data:
            confidence_metrics['overall_confidence'] = data['confidence']
        
        if 'language' in data and 'confidence' in data:
            confidence_metrics['language_confidence'] = data['confidence']
        
        if 'geographic' in data and 'confidence' in data:
            confidence_metrics['geographic_confidence'] = data['confidence']
        
        return confidence_metrics
    
    def _calculate_overall_confidence(self, data: Dict[str, Any]) -> float:
        """Calculate overall confidence score"""
        
        confidences = []
        
        if 'confidence' in data:
            confidences.append(data['confidence'])
        
        # Extract nested confidence values
        for key, value in data.items():
            if isinstance(value, dict) and 'confidence' in value:
                confidences.append(value['confidence'])
        
        return sum(confidences) / len(confidences) if confidences else 0.0
    
    def _write_csv_manually(self, data: Any, file_path: str, 
                           include_metadata: bool, metadata: Dict) -> None:
        """Manually write CSV file without pandas"""
        
        with open(file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            if include_metadata:
                writer.writerow(['# Police Cyber Intelligence Export'])
                writer.writerow(['# Export Time:', metadata['export_timestamp']])
                writer.writerow(['# Data Size:', metadata['data_size']])
                writer.writerow([])
            
            if isinstance(data, list) and data and isinstance(data[0], dict):
                # Write headers
                headers = list(data[0].keys())
                writer.writerow(headers)
                
                # Write data rows
                for item in data:
                    row = [item.get(header, '') for header in headers]
                    writer.writerow(row)
            else:
                writer.writerow(['Data'])
                writer.writerow([str(data)])
    
    def _write_xml(self, data: Any, file_path: str, 
                  include_metadata: bool, metadata: Dict) -> None:
        """Write data as XML file"""
        
        root = ET.Element('PoliceIntelligenceExport')
        
        if include_metadata:
            metadata_elem = ET.SubElement(root, 'Metadata')
            for key, value in metadata.items():
                elem = ET.SubElement(metadata_elem, key.replace('_', ''))
                elem.text = str(value)
        
        data_elem = ET.SubElement(root, 'Data')
        self._dict_to_xml(data, data_elem)
        
        tree = ET.ElementTree(root)
        tree.write(file_path, encoding='utf-8', xml_declaration=True)
    
    def _dict_to_xml(self, data: Any, parent: ET.Element) -> None:
        """Convert dictionary to XML elements"""
        
        if isinstance(data, dict):
            for key, value in data.items():
                elem = ET.SubElement(parent, str(key).replace(' ', '_'))
                self._dict_to_xml(value, elem)
        elif isinstance(data, list):
            for i, item in enumerate(data):
                elem = ET.SubElement(parent, f'item_{i}')
                self._dict_to_xml(item, elem)
        else:
            parent.text = str(data)
    
    def _generate_session_id(self) -> str:
        """Generate unique session ID for security logging"""
        
        timestamp = str(int(time.time()))
        random_component = hashlib.md5(timestamp.encode()).hexdigest()[:8]
        return f"POLICE_SESSION_{timestamp}_{random_component}"


def main():
    """Demonstration of police utility functions"""
    
    print("üèõÔ∏è POLICE CYBER MONITORING UTILITIES DEMONSTRATION")
    print("üö® Professional Functions for Law Enforcement Operations")
    print("=" * 70)
    
    # Initialize utilities
    police_utils = PoliceUtilities(log_level="INFO")
    
    print("\nüß™ TESTING CORE UTILITY FUNCTIONS:")
    print("=" * 50)
    
    # Test text preprocessing
    sample_text = "This is a test message from Mumbai about potential threats! Visit http://example.com @user123"
    cleaned_text = police_utils.text_preprocessing(sample_text)
    print(f"‚úÖ Text Preprocessing: '{sample_text[:50]}...' -> '{cleaned_text[:50]}...'")
    
    # Test language detection
    hindi_text = "‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§∏‡§Ç‡§¶‡•á‡§∂ ‡§π‡•à"
    lang_result = police_utils.language_detection(hindi_text)
    print(f"‚úÖ Language Detection: '{hindi_text}' -> {lang_result['language']} (confidence: {lang_result['confidence']:.2f})")
    
    # Test geographic extraction
    geo_text = "There was an incident in Mumbai near Delhi airport involving suspects from Bangalore"
    geo_result = police_utils.geographic_extraction(geo_text)
    print(f"‚úÖ Geographic Extraction: Found {geo_result['total_locations']} locations")
    
    # Test time analysis
    sample_timestamps = [
        "2024-01-01 10:00:00",
        "2024-01-01 10:15:00", 
        "2024-01-01 10:30:00",
        "2024-01-01 14:00:00"
    ]
    time_result = police_utils.time_analysis(sample_timestamps)
    print(f"‚úÖ Time Analysis: Analyzed {time_result.get('total_posts', 0)} posts")
    
    # Test network analysis
    sample_connections = [("user1", "user2"), ("user2", "user3"), ("user1", "user3")]
    network_result = police_utils.network_analysis(sample_connections)
    print(f"‚úÖ Network Analysis: {network_result['network_size']['total_nodes']} nodes, {network_result['network_size']['total_edges']} edges")
    
    # Test evidence formatting
    sample_data = {"threat_score": 85, "language": "english", "confidence": 0.9}
    evidence = police_utils.evidence_formatter(sample_data, "CASE_2024_001", "OFFICER_123")
    print(f"‚úÖ Evidence Formatting: Created evidence package for case {evidence['case_information']['case_id']}")
    
    # Test alert prioritization
    sample_alerts = [
        {"threat_score": 45, "keywords": ["suspicious"], "timestamp": "2024-01-01T10:00:00"},
        {"threat_score": 90, "keywords": ["bomb", "attack"], "location": "Mumbai", "timestamp": "2024-01-01T11:00:00"},
        {"threat_score": 30, "keywords": ["normal"], "timestamp": "2024-01-01T09:00:00"}
    ]
    prioritized = police_utils.alert_prioritization(sample_alerts)
    print(f"‚úÖ Alert Prioritization: Sorted {len(prioritized)} alerts by priority")
    
    # Test data export
    export_result = police_utils.data_export(sample_data, "test_export", "json")
    print(f"‚úÖ Data Export: {'Success' if export_result['success'] else 'Failed'} - {export_result.get('file_size_bytes', 0)} bytes")
    
    # Test security logging
    log_success = police_utils.security_logger(
        "SYSTEM_ACCESS", 
        {"user": "OFFICER_123", "action": "data_analysis"},
        "OFFICER_123",
        "INFO"
    )
    print(f"‚úÖ Security Logging: {'Success' if log_success else 'Failed'}")
    
    # Test performance monitoring
    performance = police_utils.performance_monitor()
    print(f"‚úÖ Performance Monitoring: System health - {performance['system_health']}")
    
    print(f"\nüìä UTILITY PERFORMANCE SUMMARY:")
    print("=" * 50)
    
    for func_name, stats in performance.get('function_statistics', {}).items():
        print(f"   {func_name}: {stats['total_calls']} calls, "
              f"{stats['average_processing_time']:.3f}s avg, "
              f"{stats['error_rate_percentage']:.1f}% errors")
    
    print(f"\nüèõÔ∏è POLICE UTILITIES SYSTEM STATUS:")
    print(f"   ‚úÖ All core functions operational")
    print(f"   üìä Performance monitoring active")
    print(f"   üîí Security audit trail maintained")
    print(f"   ‚öñÔ∏è Evidence formatting compliant")
    print(f"   üö® Alert prioritization functional")
    
    print(f"\nüéØ READY FOR LAW ENFORCEMENT DEPLOYMENT")


if __name__ == "__main__":
    main()
